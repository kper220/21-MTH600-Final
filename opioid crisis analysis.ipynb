{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1495b405",
   "metadata": {},
   "source": [
    "# Opioid Crisis - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab4549",
   "metadata": {},
   "source": [
    "I want to take a second look at the data from the Opioid Crisis datasheet.\n",
    "\n",
    "- (link here: https://www.mathmodels.org/Problems/2019/MCM-C/index.html)\n",
    "\n",
    "Motivation: was part of the MCM 2019.\n",
    "\n",
    "Data we will work with:\n",
    "* Drug identification counts in years 2010-2016\n",
    "* Socio-economic factors collected for five states (Ohio, Kentucky, West Virginia, Virginia, Pennsylvania)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f6b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# drug use data.\n",
    "df_nflis = pd.read_excel('2018_MCMProblemC_DATA/MCM_NFLIS_Data.xlsx', sheet_name=\"Data\")\n",
    "\n",
    "# socio-economic data.\n",
    "df10 = pd.read_csv('2018_MCMProblemC_DATA/ACS_10_5YR_DP02/ACS_10_5YR_DP02_with_ann.csv')\n",
    "df11 = pd.read_csv('2018_MCMProblemC_DATA/ACS_11_5YR_DP02/ACS_11_5YR_DP02_with_ann.csv')\n",
    "df12 = pd.read_csv('2018_MCMProblemC_DATA/ACS_12_5YR_DP02/ACS_12_5YR_DP02_with_ann.csv')\n",
    "df13 = pd.read_csv('2018_MCMProblemC_DATA/ACS_13_5YR_DP02/ACS_13_5YR_DP02_with_ann.csv')\n",
    "df14 = pd.read_csv('2018_MCMProblemC_DATA/ACS_14_5YR_DP02/ACS_14_5YR_DP02_with_ann.csv')\n",
    "df15 = pd.read_csv('2018_MCMProblemC_DATA/ACS_15_5YR_DP02/ACS_15_5YR_DP02_with_ann.csv')\n",
    "df16 = pd.read_csv('2018_MCMProblemC_DATA/ACS_16_5YR_DP02/ACS_16_5YR_DP02_with_ann.csv')\n",
    "\n",
    "# indexing data.\n",
    "df10_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_10_5YR_DP02/ACS_10_5YR_DP02_metadata.csv')\n",
    "df11_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_11_5YR_DP02/ACS_11_5YR_DP02_metadata.csv')\n",
    "df12_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_12_5YR_DP02/ACS_12_5YR_DP02_metadata.csv')\n",
    "df13_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_13_5YR_DP02/ACS_13_5YR_DP02_metadata.csv')\n",
    "df14_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_14_5YR_DP02/ACS_14_5YR_DP02_metadata.csv')\n",
    "df15_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_15_5YR_DP02/ACS_15_5YR_DP02_metadata.csv')\n",
    "df16_meta = pd.read_csv('2018_MCMProblemC_DATA/ACS_16_5YR_DP02/ACS_16_5YR_DP02_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd642d",
   "metadata": {},
   "source": [
    "Parts of the data are not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e508911",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "General plan: iterate over socio-economic data, and append with relevant drug use data.\n",
    "\n",
    "Feature extraction part:\n",
    "* Include geography (specifically `GEO.display-label`).\n",
    "* Exclude margin of error features.\n",
    "* Exclude columns with `(X)`.\n",
    "* Exclude non-universal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90734f45",
   "metadata": {},
   "source": [
    "The function `feature_extract` will extract features as to satisfy the above conditions (save universality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c25b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import feature_extract\n",
    "df10[feature_extract(df10, df10_meta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675f380",
   "metadata": {},
   "source": [
    "### Filtering Data with Universal Property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaddd89",
   "metadata": {},
   "source": [
    "We can only work with properties that are present for all dataframes.\n",
    "\n",
    "At the same time, the heterogenous nature of labels corresponding to the same descriptor across years prompts us to address that as well, with the following map:\n",
    "$$\\mathrm{descriptor}\\mapsto(\\mathrm{year}\\mapsto\\mathrm{label})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd345d7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import feature_index\n",
    "from opioid_crisis_lib import feature_index2\n",
    "\n",
    "# ddf = [df10, df11, df12, df13, df14, df15, df16]\n",
    "# ddf_metadata = [df10_meta, df11_meta, df12_meta, df13_meta, df14_meta, df15_meta, df16_meta]\n",
    "\n",
    "ddf_yyyy = {\n",
    "    \"2010\": df10,\n",
    "    \"2011\": df11,\n",
    "    \"2012\": df12,\n",
    "    \"2013\": df13,\n",
    "    \"2014\": df14,\n",
    "    \"2015\": df15,\n",
    "    \"2016\": df16,\n",
    "}\n",
    "ddf_metadata_yyyy = {\n",
    "    \"2010\": df10_meta,\n",
    "    \"2011\": df11_meta,\n",
    "    \"2012\": df12_meta,\n",
    "    \"2013\": df13_meta,\n",
    "    \"2014\": df14_meta,\n",
    "    \"2015\": df15_meta,\n",
    "    \"2016\": df16_meta,\n",
    "}\n",
    "\n",
    "# f_index = feature_index(ddf, ddf_metadata)\n",
    "# sorted(f_index)\n",
    "\n",
    "f_index = feature_index2(ddf_yyyy, ddf_metadata_yyyy)\n",
    "sorted(f_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4175a",
   "metadata": {},
   "source": [
    "As we see only the \"important\" data remain. We've gotten rid of error estimate data as well as inadmissible data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72503e20",
   "metadata": {},
   "source": [
    "Given a particular year we want to extract all universal relevant labels. In other words, we want the map:\n",
    "$$\n",
    "\\left(\\mathrm{descriptor}\\mapsto(\\mathrm{year}\\mapsto\\mathrm{label})\\right)\n",
    "\\mapsto\\left(\\mathrm{year}\\mapsto(\\mathrm{labels})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import label_from_feature_index\n",
    "label_from_feature_index(\"2012\", f_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_index['Estimate; ANCESTRY - Total population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c6f32",
   "metadata": {},
   "source": [
    "We obtain a map from descriptors to corresponding labels. This allows us to go back and forth between description and label.\n",
    "\n",
    "The above data corresponds to the socio-economic data of a particular county at some specified year. When we go over drug use data, the `YYYY`, `State` and `County` data should sufficiently return the appropriate socio-economic data.\n",
    "\n",
    "The following short function returns the state and county of a string as separate strings, with state written in initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import state_and_county\n",
    "state_and_county(\"Adair County, Kentucky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049ff83",
   "metadata": {},
   "source": [
    "### Retrieving Geographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dde96",
   "metadata": {},
   "source": [
    "In addition, for each county there should be a method to retrieve numerical geographic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = pd.read_csv('2021_Gaz_counties_national.txt', sep=\"\\t\")\n",
    "from opioid_crisis_lib import locate\n",
    "locate(\"ky\", \"adair\", df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_geo[[\"USPS\", \"NAME\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa876302",
   "metadata": {},
   "source": [
    "### Processing Drug Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nflis[[\"YYYY\", \"State\", \"COUNTY\", \"SubstanceName\", \"DrugReports\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470b370",
   "metadata": {},
   "source": [
    "Of importance is the type of drugs reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4902f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nflis[\"SubstanceName\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813c7c1",
   "metadata": {},
   "source": [
    "The following gives a survey of distinct drug types (indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "substanceNames = sorted(set(df_nflis[\"SubstanceName\"]))\n",
    "substanceNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(substanceNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765fac9",
   "metadata": {},
   "source": [
    "Convert this to a dictionary so we can map substance use to a particular index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "substanceNamesDict = {substanceNames[i]:i for i in range(69)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18625bc",
   "metadata": {},
   "source": [
    "Given data on `drug reports` and `substance name`, we can construct a vector which indicates extent of a particular drug use in a county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fa8b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import drug_matrix\n",
    "drug_matrix(df_nflis, substanceNamesDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47371c",
   "metadata": {},
   "source": [
    "Now due to redundancy, a single county may occur multiple times. We want to have one drug vector per county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88226838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import drug_vector\n",
    "drug_vector(\"2010\", \"oh\", \"adams\", df_nflis, substanceNamesDict, identify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bcce55",
   "metadata": {},
   "source": [
    "### Compiling Data\n",
    "\n",
    "We can do this:\n",
    "1. Determine the overall dimension of the sample matrix. (doable)\n",
    "1. Iterate through socio-economic dataframe rows, through the years 2010-2016,\n",
    "    1. For each row, gather socio-economic data, AND data which identifies the State, county and year.\n",
    "    1. Retrieve the geographic location of the county, append.\n",
    "    1. Retrieve the drug vector of the county, append.\n",
    "2. Write all appended data into one numpy array.\n",
    "3. (Optional) Move independent columns (geographical location, socio-economic data) to the beginning, and the drug vector to the end.\n",
    "\n",
    "**Note**: even with previous filtering, there are still socio-economic sample points with `(X)` terms. These terms are set to 0. (Not the best approach. we'll deal with this later. But I assume that these anomalous rows only take a small portion and are negligible.)\n",
    "\n",
    "**Note**: turns out that the drug data contains data up to 2017, which is not included in the socio-economic data. However, this new data also contains new drugs, which messes up the data (since the new-drug column is a column of zeros, which messes up pca and all that).\n",
    "\n",
    "Make sure the data is recoverable and that rows have unique identifiers. (which indicate year, state and county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a671546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from opioid_crisis_lib import generate_sample\n",
    "# sample = generate_sample(ddf_yyyy, ddf_metadata_yyyy, f_index, df_nflis, substanceNamesDict, df_geo)\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"please_dont_overwrite.csv\", sample, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2b3f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_read = pd.read_csv('please_dont_overwrite.csv', header=None)\n",
    "sample_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f712915",
   "metadata": {},
   "source": [
    "Convert to np.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0603434",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_read_np = sample_read.to_numpy()\n",
    "sample_read_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a2d83",
   "metadata": {},
   "source": [
    "### Extraction of Nonzero Columns (features) and Problematic Rows (sample points)\n",
    "\n",
    "For a column feature, we need to make sure that the column is nonzero. This is because if a column $c$ is zero, the *standardization* $$\\dfrac{c - \\mathrm{mean}\\,(c)}{\\mathrm{std}\\,(c)}$$ is undefined (in this case it is `nan`).\n",
    "\n",
    "Additionally, there still exist sample points with `nan` entries in the first and second rows (when the county location cannot be found). We will also remove those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf02c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import find_nonzero, keep_rows, keep_cols\n",
    "nonzero_index, zero_index = find_nonzero(sample_read_np.T)\n",
    "zero_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafcc0e",
   "metadata": {},
   "source": [
    "Identify the features these zero indices correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_features = np.array([\"INTPTLAT\", \"INTPTLONG                                                                                                               \"])\n",
    "features = np.concatenate((geo_features, \n",
    "                          sorted(f_index),\n",
    "                          sorted(substanceNamesDict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[zero_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccad44",
   "metadata": {},
   "source": [
    "Identify the features the nonzero indices correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_features = features[nonzero_index]\n",
    "nonzero_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c078a",
   "metadata": {},
   "source": [
    "These are drugs which are not included in the 2010-2016 dataframes. We will construct a matrix without these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801073e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample matrix with nonzero columns only.\n",
    "sample_nz_col = keep_cols(sample_read_np, nonzero_index)\n",
    "sample_nz_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983db8d",
   "metadata": {},
   "source": [
    "Next we identify all sample points with missing location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locatable indices.\n",
    "keep_row_indices = np.argwhere(~np.isnan(sample_nz_col.T[0])).T[0]\n",
    "keep_row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nz = sample_nz_col[keep_row_indices]\n",
    "sample_nz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb8682",
   "metadata": {},
   "source": [
    "Identify the sample points corresonding to the kept rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acbb1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import identify_sample_points\n",
    "sample_point_id = identify_sample_points(keep_row_indices, ddf_yyyy)\n",
    "sample_point_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0448e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_point_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90136d19",
   "metadata": {},
   "source": [
    "These will be readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_point_id_join = np.array([' '.join(sample_point_id[i]) for i in range(len(sample_point_id))])\n",
    "sample_point_id_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145fcb7",
   "metadata": {},
   "source": [
    "### Standardization.\n",
    "\n",
    "We standardize each feature column $c$: $$\\tilde{c} = \\dfrac{c - \\mathrm{mean}\\,(c)}{\\mathrm{std}\\,(c)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aaeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import standardize\n",
    "sample_standardized = standardize(sample_nz)\n",
    "sample_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a251784d",
   "metadata": {},
   "source": [
    "We can imbue this standardized matrix with its original features and identifiers (standardized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized = pd.DataFrame(sample_standardized, index=sample_point_id_join,\n",
    "                              columns=nonzero_features)\n",
    "df_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1522cb5",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "We can finally perform PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(sample_standardized, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the singular values.\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dd611",
   "metadata": {},
   "source": [
    "Plot of data wrt principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "k = 79\n",
    "\n",
    "pca = PCA(n_components=k)\n",
    "principalComponents = pca.fit_transform(df_standardized.loc[:].values)\n",
    "principalDf = pd.DataFrame(data = principalComponents,\n",
    "                           columns = ['principal component {}'.format(_k+1) for _k in range(k)])\n",
    "principalDf.index = sample_point_id_join\n",
    "\n",
    "principalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71fb70",
   "metadata": {},
   "source": [
    "Take a look at the explained variances (rounded to 3 decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(pca.explained_variance_ratio_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad1598",
   "metadata": {},
   "source": [
    "Explained variances of the first 10 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sum(pca.explained_variance_ratio_[:k+1]) for k in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc2384",
   "metadata": {},
   "source": [
    "We perform an eigen decomposition of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values, eigen_vectors = np.linalg.eig(principalDf.cov())\n",
    "eigen_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa002a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d43ca",
   "metadata": {},
   "source": [
    "Plot of sample points wrt first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(principalDf['principal component 4'], principalDf['principal component 5'],\n",
    "           alpha=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d2d83",
   "metadata": {},
   "source": [
    "Plot first three components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "xx = principalDf['principal component 1']\n",
    "yy = principalDf['principal component 2']\n",
    "zz = principalDf['principal component 3']\n",
    "\n",
    "ax.scatter(xx, yy, zz, alpha=1)\n",
    "ax.set_xlabel('principal component 1')\n",
    "ax.set_ylabel('principal component 2')\n",
    "ax.set_zlabel('principal component 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb73d9",
   "metadata": {},
   "source": [
    "So even though the first three principal components only explain approximately 40 percent of the data, it still has enough structure. In fact, by decreasing the opacity of points we see the majority of sample points follow the trajectory of principal component 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "xx = principalDf['principal component 1']\n",
    "yy = principalDf['principal component 2']\n",
    "zz = principalDf['principal component 3']\n",
    "\n",
    "ax.scatter(xx, yy, zz, alpha=.1)\n",
    "ax.set_xlabel('principal component 1')\n",
    "ax.set_ylabel('principal component 2')\n",
    "ax.set_zlabel('principal component 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "xx = principalDf['principal component 1']\n",
    "yy = principalDf['principal component 2']\n",
    "zz = principalDf['principal component 3']\n",
    "\n",
    "ax.scatter(xx, yy, zz, alpha=.1)\n",
    "ax.set_xlabel('principal component 1')\n",
    "ax.set_ylabel('principal component 2')\n",
    "ax.set_zlabel('principal component 3')\n",
    "\n",
    "ax.view_init(10, -10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83352deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "xx = principalDf['principal component 1']\n",
    "yy = principalDf['principal component 2']\n",
    "zz = principalDf['principal component 3']\n",
    "\n",
    "ax.scatter(xx, yy, zz, alpha=.1)\n",
    "ax.set_xlabel('principal component 1')\n",
    "ax.set_ylabel('principal component 2')\n",
    "ax.set_zlabel('principal component 3')\n",
    "\n",
    "ax.view_init(10, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36357570",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We can verify that the singular values agree with our eigen decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opioid_crisis_lib import threshold_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a638f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_hat = s**2/(3007-1)\n",
    "eigen_values_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pass(eigen_values_hat-eigen_values, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92373cb4",
   "metadata": {},
   "source": [
    "### The first principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_t = threshold_pass(np.matmul(u, np.diag(s))[0], .7)\n",
    "pc_t = list(pc_t)\n",
    "list(filter(lambda x: x[1] != 0, [(features[i], pc_t[i]) for i in range(len(pc_t))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a30128",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_t = threshold_pass(np.matmul(u, np.diag(s))[1], .7)\n",
    "pc_t = list(pc_t)\n",
    "list(filter(lambda x: x[1] != 0, [(features[i], pc_t[i]) for i in range(len(pc_t))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd56786",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_t = threshold_pass(np.matmul(u, np.diag(s))[2], .7)\n",
    "pc_t = list(pc_t)\n",
    "list(filter(lambda x: x[1] != 0, [(features[i], pc_t[i]) for i in range(len(pc_t))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1aedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_t = threshold_pass(np.matmul(u, np.diag(s))[3], .7)\n",
    "pc_t = list(pc_t)\n",
    "list(filter(lambda x: x[1] != 0, [(features[i], pc_t[i]) for i in range(len(pc_t))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca655e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_t = threshold_pass(np.matmul(u, np.diag(s))[4], .7)\n",
    "pc_t = list(pc_t)\n",
    "list(filter(lambda x: x[1] != 0, [(features[i], pc_t[i]) for i in range(len(pc_t))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff43cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
